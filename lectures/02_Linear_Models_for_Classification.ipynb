{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stepyt/machine_learning_notes/blob/master/lectures/02_Linear_Models_for_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFZF7gOrsqr4"
   },
   "source": [
    "# **LINEAR MODELS FOR CLASSIFICATION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LI_-1u5MDZP"
   },
   "source": [
    "## **Linear classification**\n",
    "\n",
    "The problem is similar to the one in the previous notebook but in this case the output are classes and not continuous variables. In classification, given an input $x$ we want to classify it into one of the $C_k$ classes where $k=1,...,K$. In this case there is no noise in the output. An example can be: given as input the pixel of an X-ray image we want to say if there is cancer or not. In principle we can use the same techniques used in the regression problem but at the end we will obtain really bad solutions.\n",
    "\n",
    "In linear classifcstion , the input space is divided into decision regions whose boundaries are called **decision boundaries**. In this chapter we will focus just in linear models for classification which are able to work with dataset whose classes can be separated by linear decision surfaces.\n",
    "\n",
    "Let's consider the linear regression model:\n",
    "\n",
    "$$ y(\\textbf x, \\textbf w) = w_0 + \\sum_{j=1}^{D-1} w_jx_j = \\textbf x^T \\textbf w + w_0 $$\n",
    "\n",
    "In classification we can work with categorical variable as *male* or *female*. We can convert these classes into numbers, for example the classes *male* and *female* can be converted into $0$ and $1$. The problem is that the codomain of the linear model is not $0$ and $1$ but $-\\infty$ and $+\\infty$. In this case is better to use the **generalized linear model** in which the output of the linear model is given as input to a **nonlinear function**\n",
    "\n",
    "$$y(\\textbf x, \\textbf w) = f(\\textbf x^T \\textbf w + w_0) $$\n",
    "\n",
    "The model $y$ is no more linear in the parameters since they pass thgrough a nonlinear function. Why we still call this model linear? We can answer this question by looking at the shape of the boundaries that separates the two classes into the input space. We can plot the **decision surface** by putting\n",
    "\n",
    "$$ y(\\textbf x, \\textbf w) = const $$\n",
    "\n",
    "We see that these surfaces are linear function of the input and of the parameters since correspond to hyperplane. As done for linear regression problem we can transform the input space with basis functions but we'll see it later.\n",
    "\n",
    "In classification there are different ways of representing the target values. In the two-class problem we have a binary target $t \\in \\{0, 1 \\}$ and we can think at the output as a probability distribution in which $t=1$ means that the class is $\\mathcal C_1$ and $t=0$ means that the class is $\\mathcal C_2$. This was the point o view of probabilistic models. If the class are, for example, $K$ it is more convenient to use the $1$-of-$K$ coding scheme on which $\\textbf t$ is a vector  of length $K$ that contains $1$ for the correct class and $0$ for the other. For example, if we have $5$ classes the correct class is $\\mathcal C_5$ the vector $\\textbf t$ will be $\\{0, 0, 0, 0, 1\\}$. In this case we can interpret the vector $\\textbf t$ as a probability distribution over classes.\n",
    "\n",
    "As already said in a previous chapter, there are three different approaches to classification. The simplest approach makes use of a **discriminat function**, in which we build a function that directly maps each inoput to a specific class. In a **probabilistic** approach we model the conditional probability disribution $p(\\mathcal C_k | \\textbf x)$ to make optimal decisions. We have to alternatives in a probabilistic view. \n",
    "* The *discriminative* approach model the conditional probability directly  using for example parmetric models and optimizing the parameters using a training set.\n",
    "* The *generative* approach model first the class conditional probabilities $p(\\textbf x | \\mathcal C_k)$ together with the prior of the class and thenm apply Bayes' rules:\n",
    "\n",
    "$$ p(\\mathcal C_k | \\textbf x) = \\frac{p(\\textbf x | \\mathcal C_k)p(\\mathcal C_k)}{p(\\textbf x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xS-Hf43C287l"
   },
   "source": [
    "## **Discriminant functions**\n",
    "\n",
    "Let's start considering the simplest approach in which we build a function that directly takes an input vector $\\textbf x$ and assign it to one of the $K$ classes.\n",
    "\n",
    "### **Two classes**\n",
    "\n",
    "The simplest discriminant function is obtained by taking a linear combination of the inputs\n",
    "\n",
    "$$ y(\\textbf x) = \\textbf w^T \\textbf x + w_0 $$\n",
    "\n",
    "This model assigns an input $\\textbf x$ to the class $\\mathcal C_1$ if $y(\\textbf x) \\geq 0$ and to the class $\\mathcal C_2$ otherwise. The decision boundary is therefore defined by $y(\\textbf x) = 0$ which correspond to a $(D-1)$-dimensional hyperplane in the input space.\n",
    "\n",
    "It is possible to demonstrate that the weight vector $\\textbf w$ is orthogonal to the decision surface. Let's take two different points onto the decision boundary and we have \n",
    "\n",
    "$$ y(\\textbf x_A) = y(\\textbf x_B) = 0 $$\n",
    "\n",
    "$$ \\textbf w^T(\\textbf x_A - \\textbf x_B) = 0$$\n",
    "\n",
    "The last equation is satisfied only if the weights vector is orthogonal to the line connec ting the two points. We can also prove that $w_0$ represents the location of the decision surface. If we take a point onto the decision surface we have \n",
    "\n",
    "$$ \\frac{\\textbf w^T \\textbf x}{\\left\\lVert \\textbf w \\right\\rVert_2} = - \\frac{w_0}{\\left\\lVert \\textbf w \\right\\rVert_2} $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_1.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "### **Multiple classes**\n",
    "\n",
    "To extend our model to the case of $K>2$ classeswe have different possibilities.\n",
    "\n",
    "Consider the case of *one-versus-the-rest* classifiers in which we build $K-1$ classifiers, each of which solves a two-class problem separating the class $\\mathcal C_k$ from the others\n",
    "\n",
    "An alternative is to use *one-versus-one* classifiers in which the $K(K-1)/2$ binary discriminant functions separate pairs of classes. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_2.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Is it possible to see from the two pictures above that in both the cases there are regions of ambiguity. We can solve this problem by considering a single $K$-class discriminant comprising $K$ linear functions of the form\n",
    "\n",
    "$$ y_k(\\textbf x) = \\textbf w_k^T \\textbf x + w_{k0} $$\n",
    "\n",
    "and then assigning a point to the class $ \\mathcal C_k $ if $y_k(\\textbf x) > y_j(\\textbf x) $ for all $j \\neq k$. The decision boundary between class $k$ and $j$ is therefore given by $y_k(\\textbf x) = y_j(\\textbf x)$ and hence correspond to a $(D-1)$-dimensional hyperplane. Can be easily demonstrated that the decision boundaries are always singly connected and convex. let's take two points inside the region $\\mathcal R_k$. Any point that lies on the line connecing these two points can be expressed as\n",
    "\n",
    "$$ \\hat{\\textbf x}= \\lambda \\textbf x_A + (1- \\lambda) \\textbf x_B $$\n",
    "\n",
    "Thanks to the linearity of the discriminant \n",
    "\n",
    "$$ y_k(\\hat{\\textbf x})= \\lambda y_k(\\textbf x_A) + (1- \\lambda) y_k(\\textbf x_B) $$\n",
    "\n",
    "It follows that $y_k(\\textbf x_A) > y_j(\\textbf x_A)$ and $y_k(\\textbf x_B) > y_j(\\textbf x_B)$, hence $y_k(\\hat{\\textbf x}) > y_j(\\hat{\\textbf x})$ for all $j \\neq k$ and so $\\hat{\\textbf x}$ belongs to $\\mathcal R_k$.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_3.png?raw=1\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "### ** Least squares for classification**\n",
    "\n",
    "Considering a general classification problem with $K$ classes using the $1$-of-$K$ encoding scheme we can use the least squares in order to approximate the considitional expectation $ \\mathbb E[\\textbf t| \\textbf x] $. Each class is described by its own linear model and we can conveniently write the model in matrix notation\n",
    "\n",
    "$$ \\textbf{y(x)} = \\widetilde{\\textbf W}^T \\widetilde{\\textbf x} $$\n",
    "\n",
    "where $\\widetilde{\\textbf W}$ is a $(D+1) \\times K$ matrix whose $k$-th column is $\\widetilde{\\textbf w_k} = (w_{k0}, \\textbf w_k^T)^T $ and $\\widetilde{\\textbf x} = (1, \\textbf x^T)^T$. We can determine the parameters matrix by minimizing the sum-of-squares error function. Given a dataset $\\mathcal D=\\{\\textbf x_i, t_i\\}$ where %n=1,...,N$ the least-squares solution is\n",
    "\n",
    "$$\\widetilde{\\textbf W} = (\\widetilde{\\textbf X}^T \\widetilde{\\textbf X})^{-1} \\widetilde{\\textbf X}^T \\textbf T $$\n",
    "\n",
    "where $\\textbf T$ is a $N \\times K$ matrix whose $n$-th row is the  vector $\\textbf t_n^T$  and $ \\widetilde{\\textbf X}$ is an $N \\times (D+1)$ matrix whose $i$-th row is $\\widetilde{\\textbf x}_i^T$. A new input is assigned to the class for which $t_k = \\widetilde{\\textbf x}^T \\widetilde{\\textbf w}_k$ is largest\n",
    "\n",
    "Despite the least squares method provides a closed form solution of the parmeters it lack of robustness to outliers as clearly visible from the pictures below. The green line is obtained with logistic regression (presented later) and the purple one is obtained with least-squares.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_4.png?raw=1\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "Another problem deriving from the least-squares method is that of non-Gaussian distributions. Clearly the binary target vectors have a distribution that is far from gaussian. In linear regression model we add the Gaussian distribution with the noise. In the pictures below we have on the right the classification boundaries provided by least-squares and on the right the ones provided by logistic regression. In this case, despite the three classes can be easily separated by linear boundaries the method completely fails in do this. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_5.png?raw=1\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "### **Fixed basis functions**\n",
    "\n",
    "As in the case of regression we can work in the original input space or, we can operate a nonlinear trasformation of it using a vector of *basis functions* $\\phi(\\textbf x)$. The resulting decision boundaries will be linear in the feature space $\\phi$ but will be non linear in the original input space. In this way, we can obtain a linear separation in the new space, aslo if in the original one would be impossible. Let consider the example below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_6.png?raw=1\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "The red and blue dots correspond to two different classes. Obviously, it is impossible in the figure on the left to trak a linear boundary able to separate blue and red dots. If we take two 'Gaussian' basis function, $\\phi_1(\\textbf x)$ and $\\phi_2(\\textbf x)$ we can separate the two classes easily in the figure on the right. Another approch could be to transform the original features into polar coordinates. In the new features space the horizontal axes will be the radius $\\rho$ and the vertical one the angle $\\theta$. Also in linear classification we have seen that the big challenge is to find the best way of representing the input variables.\n",
    "\n",
    "### **The perceptron**\n",
    "\n",
    "Another discriminant (not probabilistic) model for linear classification is the *perceptron* of Rosemblat (1962). This is an online algorithm, so it is able to process one sample at time and is able to work on two-class problems. The first step of the perceptron is to transform the original input space with fixed nonlinear basis functions  and use these new features to construct a generalized linear model\n",
    "\n",
    "$$y(\\textbf x) = f(\\textbf w^T \\phi(\\textbf x)) $$\n",
    "\n",
    "The nonlinear activation function $f(\\cdot)$ is given by a sign function\n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "+1, \\quad a \\geq 0 \\\\\n",
    "-1, \\quad a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So, we assign the value of $+1$ to the class $\\mathcal C_1$ and the value $-1$ to the class $\\mathcal C_2$. One possibility to learn the parameters of the model would be the minimization of the misclassified points. This approach doesn't lead to a good approach since the error function would be a piecewise constant function of $\\textbf w$. To understand this just imagine to move the decision boundary without increase or decrease the number of misclassified points, the error function will remain constant. By considering the optimization of the parameters with this method through gradient descent we will have a lot of plateau. A better choice of error function is what is called the **perceptron criterion**. We are looking for a function such that in clas $\\mathcal C_1$ we have $\\textbf w^t \\phi(\\textbf x) > 0$ and the opposite for the class $\\mathcal C_2$. If the target is $t \\in \\{-1, +1 \\}$ we would like that all the inputs satisfy $\\textbf w^T \\phi(\\textbf x_n) t_n > 0$. The perceptron algorithm assigns zero error to correctly classified inputs. The perceptron criterion uses a loss function given by\n",
    "\n",
    "$$L_P(\\textbf w) = -\\sum_{n \\in \\mathcal M} \\textbf w^T \\phi_n t_n$$\n",
    "\n",
    "where $\\mathcal M$ represents the set of all misclassified points. In this way the error function is piecewise linear in $\\textbf w$. The optimization of the parameters is obtained with Stochastic Gradiend Descent (SGD) as follow\n",
    "\n",
    "$$ \\textbf w^{(k + 1)} = \\textbf w^{(k)} - \\alpha \\nabla L_P(\\textbf w) = \\textbf w^{(k)} + \\alpha \\phi(\\textbf x_n) t_n$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. Since multiplying $\\textbf w$ by a constant term the perceptron function does not change we can set the learning rate equal to one. Below is reported the pseudo code of the perceptron.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_7.png?raw=1\" width=\"200\">\n",
    "</p>\n",
    "\n",
    "An update example of the perceptron algorithm is illustrated in the figures below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_8.png?raw=1\" width=\"200\" hspace=\"20\"> <img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_9.png?raw=1\" width=\"200\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_10.png?raw=1\" width=\"200\" hspace=\"20\"> <img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_11.png?raw=1\" width=\"200\">\n",
    "</p>\n",
    "\n",
    "The black arrow represents the actual parameter vector $\\textbf w$ while the point with a green circle around is the misclassified one. The feature vector of the misclassified point is added to the current weight vector  giving the new decision boundary. Since the change in weight vector may cause some previously correctly classified points to be misclassified , the perceptron ruel doesn't guarantee to reduce the total error at each step. The effect of a single update is just to reduce the error due to the misclassified pattern. \n",
    "\n",
    "$$ - \\textbf w^{(k+1)T} \\phi_n t_n = - \\textbf w^{(k)T} \\phi_n t_n - (\\phi_n t_n)^T\\phi_n t_n < - \\textbf w^{(k)T} \\phi_n t_n $$\n",
    "\n",
    "The *perceptron convergence theorem* states that if there exists an exact solution, so if ther training set is linearly separable in the featur space $ \\bf \\Phi $, then the perceptron learning algorithm is guaranteed to find the exact solution in a finite number of steps. However, the convergence can be really slow and we may not be able to distinguish between nonseparable problems and slowly converging ones. If multiple solutions exist, the one find at convergence depends on the initialization of the parameters and on the order of presentation of the data points.\n",
    "For dataset that are not linearly separable the perceptron will never converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wi-Ygw287m"
   },
   "source": [
    "## **Probabilistic discriminative approach**\n",
    "\n",
    "### **Logistic regression**\n",
    "\n",
    "We will now take a step forward considering one of the most used algorithms for classification. Consider first of all the case of two classes and the posterior probability for the first class\n",
    "\n",
    "$$ p(\\mathcal C_1 | \\textbf x) = \\frac{p(\\textbf x | \\mathcal C_1) p(\\mathcal C_1)}{p(\\textbf x | \\mathcal C_1) p(\\mathcal C_1) + p(\\textbf x | \\mathcal C_2) p(\\mathcal C_2)} =\n",
    "\\frac{1}{1 + exp(-a)} = \\sigma(a) $$\n",
    "\n",
    "where $\\sigma(a)$ is called *logistic sigmoid function*. We can thus write the posterior of class $\\mathcal C_1$ as a logistic sigmoid acting on a linear function of the feature vector\n",
    "\n",
    "$$p(\\mathcal C_1 | \\phi) = y(\\phi) = \\sigma(\\textbf w^T \\phi) $$\n",
    "\n",
    "and consequently $p(\\mathcal C_2 | \\phi) = 1 - p(\\mathcal C_1 | \\phi)$. This is a generalized linear model since, as in previous cases, the boundary is defined by $\\sigma(\\textbf w^T \\phi) = const$. Despite the logistic sigmoid function is used for classification this model is called *logistic regression*. The shape of the logistic sigmoid is depicted in the figure below\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_12.png?raw=1\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "The parameters of the logistic regression model are determined through maximum likelihood. Given a dataset $\\mathcal D = \\{\\textbf x_n, t_n\\}$ for $n=1,...,N$ and $t \\in \\{0, 1\\}$, the likelihood function is given by\n",
    "\n",
    "$$ p(\\textbf t | \\textbf X, \\textbf w) = \\prod_{n=1}^Ny_n^{t_n}(1 - y_n)^{1-t_n} $$\n",
    "\n",
    "where $y_n = \\sigma(\\textbf w^T \\phi_n)$. We can define the error function by taking the negative log of the likelihood, obtaining the **cross entropy error function**.\n",
    "\n",
    "$$ L(\\textbf w) = - ln p(\\textbf t | \\textbf X, \\textbf w) = - \\sum_{n=1}^N(t_n \\ln y_n - (1 - t_n) \\ln(1 - y_n)) = \\sum_{n=1}^N L_n$$\n",
    "\n",
    "In this case there is no a closed form solution, due to the nonlinearity of the sigmoid, but we can compute the gradient of the loss function w.r.t. the parameters obtaining\n",
    "\n",
    "$$ \\nabla L(\\textbf w) = \\sum_{n=1}^N(y_n - t_n) \\phi_n $$\n",
    "\n",
    "We can notice that it takes exactly the same form as the gradient of the sum-of-squares error function for the linear regression model. The error function is convex, so a global optimum exists and can be found with gradient-based optimization and adopting online methods.\n",
    "\n",
    "> **NOTE: Deriving logistic regression**\n",
    ">\n",
    "> The idea is that we want to use the same linear equation to compute probability of two classes. First of all, in binary classification the output can be $0$ or $1$ but in a linear model it can goes from $-\\infty$ to $+\\infty$. Let's bound the linear equation\n",
    "> $$ y(\\textbf w) = \\textbf w^T \\textbf x $$\n",
    "> between $0$ and $1$ in order to have a valid probability function.\n",
    "> $$ p = \\frac{e^y}{e^y + 1} $$\n",
    "> Let's take now the odds ratio, or rather the ratio between the probability of the positive event and the probability of the negative one\n",
    "> $$ odds(p) = \\frac{p}{1 - p} = e^y $$\n",
    "> By taking the log of the odds we have\n",
    "> $$ \\log\\Big(\\frac{p}{1-p}\\Big) = y $$\n",
    "\n",
    "### **Multiclass logistic regression**\n",
    "In the multiclass case the posterior probabilities is represented through the generalization  of the logistic sigmoid called **softmax function**. This name comes from the fact that the function is a smoothed verion of the 'max' function. The posterior can be written as\n",
    "\n",
    "$$ p(\\mathcal C_k | \\phi) = y_k(\\phi) = \\frac{exp(\\textbf w_k^T \\phi)}{\\sum_j exp(\\textbf w_j^T \\phi)} $$\n",
    "\n",
    "The denominator guarantees that the probability will be bounded between $0$ and $1$. At this point we compute the value of the parameters directly with maximum likelihood. It is easy to write down the likelihood using the $1$-of-$K$ coding scheme where the target is bianry\n",
    "\n",
    "$$ p(\\textbf T | \\textbf w_1, ..., \\textbf w_K) = \\prod_{n=1}^N \\prod_{k=1}^Kp(\\mathcal C_k | \\phi_n)^{t_{nk}} = \\prod_{n=1}^N \\prod{k=1^K y_{nk}^{t_{nk}} $$\n",
    "\n",
    "where $\\textbf T$ is a $N \\times K$ matrix of target variables and\n",
    "\n",
    "$$ y_{nk} = p(\\mathcal C_k | \\phi_n) = \\frac{exp(\\textbf w_k^T \\phi)}{\\sum_j exp(\\textbf w_j^T \\phi)} $$\n",
    "\n",
    "Taking the negative logarithm gives the **cross-entropy function**\n",
    "\n",
    "$$ L(\\textbf w_1, ..., \\textbf w_K) = - \\ln p(\\textbf T | \\textbf w_1, ..., \\textbf w_K) = - \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\ln y_{nk} $$\n",
    "\n",
    "Taking the gradient w.r.t. a generic parameter vector, for example $\\textbf w_j$, we obtain\n",
    "\n",
    "$$ \\nabla _{\\textbf w_j} E(\\textbf w_1, ..., \\textbf w_K) = \\sum_{n=1}^N(y_{nj} - t_{nj}) \\phi_n $$\n",
    "\n",
    "Once again, we found the same form that was found for the sum-of-squares error function with the linear model and the cross-entropy for the logistic regression. Also in this case we can apply a sequential algorithm for the learning\n",
    "\n",
    "As one can observe from the plot of the logistic function, it is very similar to the step function\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict3/figure3_13.png?raw=1\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "The main difference is that the logistic is smoother and this allows to remove the instability related to nonlinearly separable variables in the input space which means that the algorithm does not converge. The logistic regression will converge also in the case of nonlinearly separable features in point in which there will be some misclassified value.\n",
    "\n",
    "> With this notebook we have finished all the *basic* tools of machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGY6LMQ5287m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Linear Models for Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
