{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dmfh9_uqjhsj"
   },
   "source": [
    "# **LINEAR MODELS FOR REGRESSION**\n",
    "\n",
    "In this chapter we start to learn some fundamental algorithms behind the most common and simple machine larning tools. To do it, we will use some simple mathematic equations that require basic knowledge of statistics and propability. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/funny/mlhype.PNG?raw=1\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bF9kgapnjoj0"
   },
   "source": [
    "## **Linear models**\n",
    "\n",
    "The goal of a regression problem is to learn a mapping from a given $D$-dimensional vector **x** of input variables to one or more continuous target variables **t**. Some examples of regression problems are the prediction of a stock market price, the prediction of future emissions of $CO_2$, the effect of an actuation in robotics and so on.\n",
    "\n",
    "The simplest form of regression is given by **linear models** which are models linear in their weights, not necessarly in the inputs. The advantages of these models are many:\n",
    "\n",
    "* can be solved analytically. This means, as you will see later, that the associated optimization problem can be solved in _closed form_. We will always obtain the minimum of the approximation loss on the data we have. If the model fails to properly approximate the phenomenon then the problem is for sure related to the hypothesis space, so on the model we are using.\n",
    "\n",
    "* they are the core concepts of more complex machine learning algorithms.\n",
    "\n",
    "* their approximation capability is not limited to linear function: they can approximate any complex function provided the correct input space is considered.\n",
    "\n",
    "The simplest linear model, is obviously, the one that involves a linear combination of the input variables:\n",
    "\n",
    "$$ y(\\textbf{x}, \\textbf{w}) = w_0 + \\sum_{j=1}^{D-1}w_jx_j = \\textbf{w}^T\\textbf{x} $$\n",
    "\n",
    "where $w_0$ is nothing but the offset of the model at zero.\n",
    "\n",
    "### **Loss function**\n",
    "\n",
    "How can we state if this model is sufficently accurate for our system? We need to define a loss function, also called the _error function_. Generally speaking a loss function can be defined as:\n",
    "\n",
    "$$ \\mathbb{E}[\\mathcal{L}] = \\int\\int L(t, y(\\textbf{x}))p(\\textbf{x},t)d\\textbf{x}dt $$\n",
    "\n",
    "where $p(\\textbf{x}, t)$ represents the probability to see that particular combination of input and output; this probability acts as a weight on the loss. Since the distribution of this probability is unknown we have to estimate it with the data points we have.\n",
    "\n",
    "In regression, a common choice of loss function is the **squared loss function**:\n",
    "\n",
    "$$ \\mathbb{E}[\\mathcal{L}] = \\int\\int (t - y(\\textbf{x}))^2p(\\textbf{x},t)d\\textbf{x}dt $$\n",
    "\n",
    "Deriving this loss function w.r.t. $y$ and setting the derivative to zero (passages in [chapter 1.5.5](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)) we obtain that the optimal solution is the conditional mean of $t$ conditioned on $\\textbf{x}$, also called *regression function*:\n",
    "\n",
    "$$ y(\\textbf{x}) = \\int t p(t|\\textbf{x})dt = \\mathbb{E}[t|\\textbf{x}] $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_1.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "There are situations in which the squared loss function doesn't produce any good result, as in the case where $p(t|\\textbf{x})$ is multimodal. The generalization of the squared loss is the *Minkowski loss* defined as:\n",
    "\n",
    "$$\\mathbb{E}[\\mathcal{L}] = \\int\\int (t - y(\\textbf{x}))^qp(\\textbf{x},t)d\\textbf{x}dt $$\n",
    "\n",
    "Up to now we have only looked at linear functions of weights and inputs. In order to approximate more complex non-linear functions we can use *non-linear basis functions*. In this way the model still remains linear in the weights but it will be non linear in the input features.\n",
    "\n",
    "$$ y(\\textbf{x}, \\textbf{w}) = w_0 + \\sum_{j=1}^{M-1}w_j\\phi_j(\\textbf{x}) = \\textbf{w}^T \\bf{\\phi}(x) $$\n",
    "\n",
    "Thanks to $\\bf{\\phi}$ we transorm the original input space into another one. By using nonlinear basis functions, we allow the function $y(\\textbf{x}, \\textbf{w})$ to be a nonlinear function of the input vector. In this way we can exploit also our domain knowledge by providing a specific transormation of the inputs.\n",
    "\n",
    "Some examples of basis functions are:\n",
    "\n",
    "* Polynomial\n",
    "\n",
    "* Gaussian\n",
    "\n",
    "* Sigmoidal\n",
    "\n",
    "* Fourier basis \n",
    "\n",
    "The pictures below show the Gaussian basis functions and the Sigmoid ones.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_2bis.png?raw=1\" width=\"550\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "miO_Dck_2Yht"
   },
   "source": [
    "> **REGRESSION APPROACHES:**\n",
    ">\n",
    "> <span style='color:SlateBlue;font-style:italic'>**Generative**</span>: these are the models that explicitly model the distribution of the inputs as well as the outputs. Probabilistically speaking they model also $p(\\textbf{x},t)$\n",
    ">\n",
    "> <span style='color:SlateBlue;font-style:italic'>**Discriminative**</span>: they directly model the posterior probability $p(t|\\textbf{x})$.\n",
    ">\n",
    "> <span style='color:SlateBlue;font-style:italic'>**Direct**</span>: they directly find a mapping function from each input to the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXtr8w86xZaL"
   },
   "source": [
    "## **Minimizing Least Squares**\n",
    "\n",
    "Let's consider a data set of inputs $\\textbf{X}=\\{\\textbf{x}_1,...,\\textbf{x}_N\\}$ with the associated target values $t_1,...,t_N$. The error function is defined as the *sum of squared estimate of errors (SSE)*:\n",
    "\n",
    "$$ SSE(\\textbf{w}) = \\mathcal{L}(\\textbf{w}) = \\frac{1}{2} \\sum_{n=1}^N(y(\\textbf{x}_n, \\textbf{w}) - t_n)^2 $$\n",
    "\n",
    "The loss can also be written in terms of the $l_2$ norm of the residual errors, known as *residual sum of squares (RSS)*:\n",
    "\n",
    "$$ RSS(\\textbf{w}) = \\left\\lVert  \\epsilon \\right\\rVert ^2_2 = \\sum^N_{n=1} \\epsilon_n^2 $$\n",
    "\n",
    "Note that if we have an high number $N$ of samples the approximation will be very good otherwise, if $N$ is small, the loss will be very noisy. This is because, without the knowledge of $p(\\textbf x, t)$ we have to approximate it with a finite number of points and, higher is the number of points and better is the approximation.\n",
    "\n",
    "In matricial form the RSS becomes:\n",
    "\n",
    "$$ \\mathcal{L}(\\textbf{w}) = \\frac{1}{2}(\\textbf{t} - \\bf{\\Phi}\\textbf{w})^T(\\textbf{t} - \\bf{\\Phi}\\textbf{w})$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\bf{\\Phi} = (\\phi(\\textbf{x}_1), ...,\\phi(\\textbf{x}_N))^T  $$\n",
    "\n",
    "To obtain the minimum of the loss we calculate its the first and the second derivatives w.r.t. the weights:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\textbf{w}} = -\\bf{\\Phi}^T(\\textbf{t} - \\bf{\\Phi}\\textbf{w}) $$\n",
    "\n",
    "$$ \\frac{\\partial^2 \\mathcal{L}(\\textbf{w})}{\\partial \\textbf{w}\\partial \\textbf{w}} =  \\bf{\\Phi}^T\\bf{\\Phi}$$\n",
    "\n",
    "Providing that the Gram matrix (or Gramian matrix, or *Gramian*) $\\bf{\\Phi}^T\\bf{\\Phi}$ is nonsingular, the solution of the *ordinary least squares (OLS)* is:\n",
    "\n",
    "$$ \\hat{\\textbf{w}}_{OLS} = (\\bf{\\Phi}^T\\bf{\\Phi})^{-1}\\bf{\\Phi}^T\\textbf{t} $$\n",
    "\n",
    "This set of equations are known as *normal equations* for the least squares problem. The inversion of the Gram matrix can be performed with Singular Values Decomposition (SVD) at the complexity cost of $\\mathcal{O}(NM^2 + M^3)$, where $M$ is the number of features.\n",
    "\n",
    "### **Sequential learning**\n",
    "\n",
    "Batch techniques can be computationally expensive when dealing with big data. In order to solve this problem we can rely on sequential algorithms, also known as *online algorithms*. This type of updates are also helpful for real-time applications where the data arrives in a continuous stream. Since the total loss can be expressed as the sum of the loss in each data point we can update the parameters with **stochastic gradient descent (SGD)** as follow:\n",
    "\n",
    "$$ \\textbf{w}^{k+1} = \\textbf{w}^{k} - \\alpha^{k+1}\\nabla\\mathcal{L}(x_n) $$\n",
    "\n",
    "where k represents the iteration number and $\\alpha$ is called the learning rate. The value of $\\alpha$ needs to satisfy certain conditions for convergence:\n",
    "\n",
    "$$ \\sum^{\\infty}_{k=0}\\alpha^k = + \\infty $$\n",
    "\n",
    "$$ \\sum^{\\infty}_{k=0}(\\alpha^k)^2 < + \\infty $$\n",
    "\n",
    "### **Geometric interpretation**\n",
    "\n",
    "Let's consider an $N$-dimensional space which axes are given by the $t_n$. Each basis function can also be represented as a vector in the same space. Define $\\varphi_j$ the $j^{th}$ column of $\\Phi$. If the number $M$ of basis functions is smaller than the number $N$ of data points, then the $\\varphi_j$ will span just a linear subspace of dimensionality $M$. Since $\\textbf{y}$ is defined as a linear combination of the vectors $\\varphi_j$ it lives in the same space of dimension $M$. The least squares solution for $\\textbf{w}$ corresponds to the solution $\\hat{\\textbf t}$ ($\\textbf{y}$ in the picture below) that is closest to $\\textbf{t}$ in the subspace.\n",
    "\n",
    "$$ \\hat{\\textbf{t}} = \\bf{\\Phi}\\hat{\\textbf{w}} = \\bf{\\Phi}(\\bf{\\Phi}^T\\bf{\\Phi})^{-1}\\bf{\\Phi}^T \\textbf{t}$$\n",
    "\n",
    "The solution corresponds to the projection of $\\textbf{t}$ in the subspace of dimensionality $M$ through a projection matrix called the *hat matrix*:\n",
    "\n",
    "$$ H = \\bf{\\Phi}(\\bf{\\Phi}^T\\bf{\\Phi})^{-1}\\bf{\\Phi}^T $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_4.png?raw=1\" width=\"400\" hspace=\"20\">\n",
    "</p>\n",
    "\n",
    "### **Maximum likelihood**\n",
    "Let's consider the relationship between the least squares approach and the maximum likelihood approach. We assume that the target $t$ is in a deterministic relationship with $\\textbf{x}$ and $\\textbf{w}$ contamined with additive noise:\n",
    "\n",
    "$$ t = y(\\textbf{x},\\textbf{w}) + \\epsilon $$\n",
    "\n",
    "We assume a Gaussian model for the noise function $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2) = \\mathcal{N}(0, \\beta^{-1}) $\n",
    "\n",
    "where $\\sigma^2$ is the variance and $\\beta$ is the precision. Given N samples independent and identically distributed (iid) we can write the *likelihood function* <a name=\"likelihood\"></a>:\n",
    "\n",
    "$$ p(\\textbf{t}|\\textbf{X},\\textbf{w},\\sigma^2) = \\prod^N_{n=1}\\mathcal{N}(t_n|\\textbf{w}^T\\phi(\\textbf{x}_n), \\sigma^2)  $$\n",
    "\n",
    "> **NOTE**\n",
    "> $$ \\mathcal{N}(x| \\mu, \\sigma^2) = \\frac{1}{(2 \\pi \\sigma)^{1/2}} \\exp\\Big\\{-\\frac{1}{2\\sigma^2}(x- \\mu)^2\\Big\\}  $$\n",
    "\n",
    "Taking the logarithm of the likelihood function ([chapter 1.2.4](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)):\n",
    "\n",
    "$$ \\ell(\\textbf{w}) = \\ln p(\\textbf{t}|\\textbf{X},\\textbf{w},\\sigma^2) = \\sum^N_{n=1}\\ln p(t_n|\\textbf{w},\\textbf{x}_n, \\sigma^2) \\\\ =-\n",
    "\\frac{N}{2}\\log (2 \\pi \\sigma^2) -\\frac{1}{2 \\sigma^2} RSS(\\textbf{w}) $$\n",
    "\n",
    "To find the maximum likelihood we just have to set to zero the derivative of the likelihood w.r.t. the weights:\n",
    "\n",
    "$$ \\nabla \\ell(\\textbf{w}) = \\sum^N_{n=1}t_n \\phi(\\textbf{x}_n)^T - \\textbf{w}^T \\sum^N_{n=1} \\phi(\\textbf{x}_n) \\phi(\\textbf{x}_n)^T = 0$$\n",
    "\n",
    "from which we obtain the *maximum likelihood* of the weights:\n",
    "\n",
    "$$ \\textbf{w}_{ML} = (\\bf{\\Phi}^T \\bf{\\Phi})^{-1} \\bf{\\Phi}^T \\textbf{t} $$\n",
    "\n",
    "Notice that the maximum likelihood values of the parameters don't depend on the variance of the additive noise.\n",
    "\n",
    "Assuming that the observation $t_i$ are uncorrelated and with constant variance $\\sigma^2$ and that the $x_i$ are non random, the variance of the least squares estimate is:\n",
    "\n",
    "$$ Var(\\hat{ \\textbf{w} }_{OLS}) = (\\bf{\\Phi}^T \\bf{\\Phi})^{-1} \\sigma^2 $$\n",
    "\n",
    "We can notice that the value of the variance decreases as the eigenvalues of $(\\bf{\\Phi}^T \\bf{\\Phi})$ increase. The inverse produces low values when the eigenvalues of $\\bf{\\Phi}^T \\bf{\\Phi} $ are high so when the features are linearly independent and the number of samples is high.\n",
    "\n",
    "> **NOTE**\n",
    "> $$ A = diag(\\lambda_i) \\rightarrow A^{-1} = diag(\\lambda_i^{-1}) $$\n",
    "\n",
    "Usually the variance is estimated as:\n",
    "\n",
    "$$ \\hat{\\sigma}^2  = \\frac{1}{N-M-1} \\sum^N_{n=1}(t_n - \\hat{\\textbf{w}}^T \\phi(\\textbf{x}_n))^2$$\n",
    "\n",
    "So, we obtain that:\n",
    "\n",
    "$$ \\hat{\\textbf{w}} \\sim \\mathcal{N}(\\textbf{w}, (\\bf{\\Phi}^T \\bf{\\Phi})^{-1} \\sigma^2) $$\n",
    "\n",
    "\n",
    "> **GAUSS-MARKOV THEOREM**\n",
    ">\n",
    "> The least squares estimate of $\\textbf{w}$ has the smallest variance among all linear unbiased estimates.\n",
    "\n",
    "This theorem tells us that the least squares estimator has the lowest Mean Squared Error (MSE) among all the linear estimators with no bias. However, there may exists a biased estimator with smaller MSE. *An unbiased estimator is not always the best choice.*\n",
    "\n",
    "\n",
    "### **Multiple outputs**\n",
    "Let's now consider the case of multiple outputs. In this case we have the possibility to use different sets of basis functions for each output, obtaining different and independent regression problems. However, the most common approach is to use the same set of basis functions to model all the components of the target vector.\n",
    "\n",
    "$$ \\textbf{y(x,w)} = \\textbf{W}^T\\phi(\\textbf{x}) $$\n",
    "\n",
    "where $\\textbf{y}$ is a $K$-dimensional column vector, $\\textbf{W}$ is an $M \\times K$ matrix and $\\phi(\\textbf{x})$ is an $M$-dimensional column vector.\n",
    "\n",
    "With the same procedure seen before, the maximum likelihood approach, we obtain the values of the parameters:\n",
    "\n",
    "$$ \\hat{\\textbf{W}}_{ML} = (\\bf{\\Phi}^T \\bf{\\Phi})^{-1} \\bf{\\Phi}^T \\textbf{T} $$\n",
    "\n",
    "For each output $t_k$ we have:\n",
    "\n",
    "$$ \\hat{\\textbf{w}}_{k} = (\\bf{\\Phi}^T \\bf{\\Phi})^{-1} \\bf{\\Phi}^T \\textbf{t}_k = \\bf{\\Phi}^{\\dagger} \\textbf{t}_k $$\n",
    "\n",
    "where $\\bf{\\Phi}^{\\dagger}$ is called the *Moore-Penrose pseudo-inverse* and needs to be computed only once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_wSutV9jU93"
   },
   "source": [
    "## **Regularization**\n",
    "\n",
    "In order to have a clear idea of what is and why we need regularization the best way is considering an example: the polynomial curve fitting. Let's consider the function $sin(2 \\pi x)$ and draw 10 samples of this function with random noise included. Now we want to fit these points with a polynomial function of the form:\n",
    "\n",
    "$$y(x, \\textbf{w}) = w_0 + w_1x + ... + w_Mx^M = \\sum_{j=0}^M w_jx^j $$\n",
    "\n",
    "$M$ represents the order of the polynomial. For various values of $M$ we obtain different fitting of the data points:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_5bis.png?raw=1\" width=\"500\">\n",
    "</p>\n",
    "    \n",
    "The problem is choosing the value of M: this is a very important concept called **model selection**. With a low order polynomial what we obtain is an **underfitting** of the data while, with an high order polynomial, we obtain an **overfitting**. In this last case the model starts to learn the *idiosyncratic* noise of the data set at hand.\n",
    "\n",
    "* <span style='color:SlateBlue;font-style:italic;font-weight:bold'> Overfitting: </span>the model is too strong and learn the position of the data points inside the training set without discovering the underlying function.\n",
    "\n",
    "* <span style='color:SlateBlue;font-style:italic;font-weight:bold'> Underfitting: </span>the model is too weak and is not able to approximate well the points inside the training set.\n",
    "\n",
    "\n",
    "Since the objective of regression, and in general of ML, is the generalization to unseen samples we need a model in the middle of the two extremes. In order to evaluate the overfitting we generate another set composed by $100$ samples, taken from the same distribution of the $10$ samples generate before. To have some quantitative insight into the generalization performance we evaluate the loss both on the initial data set, called *training set*, and also on the new one, called *test set*. In this case we evaluate the Root Mean Square (RMS) error since it allows to account for the difference in size of the two bunch of points.\n",
    "\n",
    "$$ E_{RMS} = \\sqrt{\\frac{2 \\cdot RSS}{N}} $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_9.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "The *test set error* is a measure of how well the model will behave on unseen samples. We can see from the above pictures that for $M=9$ the train error goes to zero but the error on the test set increases considerably. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/funny/itsok.JPG?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "This is because as $M$ increases  the magnitude of the coefficients gets larger, producing wiggling curves. Wiggling curves can be perfectly tuned to pass through all the points of the training set, as in the case of $M=9$, but that's not we want. We know that the points used to optimize the coefficient derive from a function soiled with noise so, we prefer a function that does not interpolate precisely the training points encouraging a smoother interpolation. Below are reported the values of the parameters for various order of the polynomial model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_10.png?raw=1\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "One possibilty to overcome overfitting problems is to introduce a regularization term inside the loss function which prevents the coefficients to reach too high values.\n",
    "\n",
    "### **Ridge regression**\n",
    "In ridge regression the loss is modified as follows:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\mathcal{L}(\\textbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N(y(\\textbf{x}_n, \\textbf{w}) - t_n)^2 + \\frac{\\lambda}{2} \\left\\lVert \\textbf{w} \\right\\rVert_2^2 = \\\\ &= \\mathcal{L}_D(\\textbf{w}) + \\lambda \\mathcal{L}_W(\\textbf{w})\n",
    "\\end{align*} $$\n",
    "\n",
    "where the first term is nothing but the standard RSS while the second term affects the complexity of the model. In ML this choice of regularizer is known as *weights decay*. In statistics this is an example of *parameters shrinkage* since it shrinks the parameter values toward zero. We see that the magnitude of the weights enter directly in the loss function, since we want to minimize the loss we need to converge to a solution with small values of $\\textbf w$.\n",
    "\n",
    "The advantage of this technique is that the loss function remains quadratic on the weights and a closed form solution is possible:\n",
    "\n",
    "$$ \\textbf w = (\\lambda \\textbf I + \\bf \\Phi ^T \\bf \\Phi)^{-1} \\bf \\Phi^T \\textbf t $$\n",
    "\n",
    "The effect of $\\lambda$ is to virtually reduce the hypothesis space. In this case the matrix is always invertible if the regularization term is not null and it gives a lowerbound on the eigenvalues. We can have two cases:\n",
    "\n",
    "* $\\lambda=0$: we have the OLS which is unbiased but has high variance\n",
    "* $\\lambda \\ne 0$: we introduce a bias but the variance is reduced\n",
    "\n",
    "It is important to remark that the bias is necessary when we don't have enough data in order to prevent from overfitting. We can see from the pictures below that as $\\lambda$ increases the shape of the approximating function becomes smoother.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_11bis.png?raw=1\" width=\"550\">\n",
    "</p>\n",
    "\n",
    "We can quantify the effect of the regularization by looking at the loss on the train and on the test set.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_15.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "### **Lasso regression**\n",
    "Lasso is another popular technique in which the loss is modified as:\n",
    "\n",
    "$$ \\mathcal{L}(\\textbf{w}) = \\frac{1}{2} \\sum_{n=1}^N(y(\\textbf{x}_n, \\textbf{w}) - t_n)^2 + \\frac{\\lambda}{2} \\left\\lVert \\textbf{w} \\right\\rVert_1 $$\n",
    "\n",
    "The important effect of Lasso is that if $\\lambda$ is sufficiently large some parameters goes to zero producing a sparse model (easier to interpret) and can be used also for features selection. We can see this by thinking that the Lasso method can be seen as an unregularized minimization problem subjected to the constraint:\n",
    "\n",
    "$$ \\sum^M_{j=1} \\left\\lVert w_j \\right\\rVert < \\eta $$\n",
    "\n",
    "The origin of the sparsity of Lasso can be better understand by looking at the following pictures\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_16.png?raw=1\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "The red circles are the isolines of the unregularized loss function while the light blue parts are the regions of the weights-space that satify the constraints.\n",
    "\n",
    "In the Ridge regression technique the $l_2$ norm squared distance produces an hyper-sphere region (that is a circle in the 2D space) centered in the origin. Reducing $\\lambda$ the circle radius reduces as well.\n",
    "In the Lasso technique the constraint is a polytope (that is a rhomboid in the 2D space). Reducing $\\lambda$ the constraint space wil reduce as well, but differently from Ridge Regression, $w^*$ can be located exactly on one axis (so that we will have a weight equal to 0).\n",
    "\n",
    "Regularization allows to train complex models also with few data without obtaining severe overfitting. The problem of determining the maximum number of the polynomial degree is translated into determining the best value of the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uM7s1Le5G6LB"
   },
   "source": [
    "## **Bayesian linear regression**\n",
    "\n",
    "What we have seen so far was the frequentist approach to linear regression. We have data and we want to find the parameters that better explain the data. In the Bayesian approach the perspective changes. Here the parameters are *uncertain* and they become random variables. We can specify our knowledge on the parameters by specifying a prior distribution based on what we know about the problem. We start from a *prior* distribution, we observe data and these data change our prior distribution. Combining prior and data we obtain a new probability over parameters called *posterior*. We change our knowledge by observing the real world. The posterior can be used for make prediction and the output will be a probability distribution over all the parameters and not a single value. In this case we can account for uncertainties and the presence of the uncertainties helps to make decisions about the result.\n",
    "\n",
    "The **posterior** can be found by combining the **prior** with the [**likelihood**](#likelihood) of the parameters (given data) through the **Bayes' Rule**:\n",
    "\n",
    "$$ p(parameters|data) = \\frac{p(data|parameters)p(parameters)}{p(data)} $$\n",
    "\n",
    "$$ posterior = \\frac{likelihood\\times prior}{marginal} $$\n",
    "\n",
    "$$ p(\\textbf w | D) = \\frac{p( \\mathcal D | \\textbf w)p(\\textbf w)}{p(\\mathcal D)} $$\n",
    "\n",
    "where $\\textbf w$ are the weights of the model and $ \\mathcal D$ represents the observed data.\n",
    "\n",
    "If we don't have a priori knowledge about parameters distribution we can provide an uniformative prior. This prior is nothing but a uniform probability over all the parameters space. \n",
    "\n",
    "Since the denominator is just a normalizing term we can write that the posterior is proportional to product of the likelihood and of the prior:\n",
    "\n",
    "$$ posterior \\propto likelihod \\times prior $$\n",
    "\n",
    "If we want to have a single output from our Bayesian model we can take the most probable value of $\\textbf w$ given the data, the mode of the posterior. This approach is called **maximum a posteriori (MAP)**.\n",
    "\n",
    "The Bayesian approach is another method that allows to avoid overfitting since the prior on the parameters acts as a regularizator. The stronger is the prior and less the parameters are free to move in the design space. We can imagine the prior like a spring centered in the mean of the distribution with a stiffness constant $k$ proportional to the variance. The drawback is that if our prior is wrong we add an high bias to the model.\n",
    "\n",
    "Assuming a multivariate Gaussian prior distribution\n",
    "\n",
    "$$ p(\\textbf w) = \\mathcal{N}( \\textbf w | \\textbf w_0, \\textbf S_0), $$\n",
    "\n",
    "we are specifying that from what we know the mean of the parameters is given by $\\textbf w_0$ with a covariance matrix given by $ \\textbf S_0$.\n",
    "\n",
    "If also the likelihood is Gaussian then the posterior will be Gaussian too  (this is true only for Gaussian probability distributions).\n",
    "\n",
    "$$ p(\\textbf w | \\textbf t , \\bf \\Phi, \\sigma^2) \\propto \\mathcal N (\\textbf w | \\textbf w_0, \\textbf S_0) \\mathcal N (\\textbf t| \\bf \\Phi \\textbf w, \\sigma^2 \\textbf I_N) = \\mathcal N (\\textbf w | \\textbf w_N, \\textbf S_N) $$\n",
    "\n",
    "$$ \\textbf w_N = \\textbf S_N \\Big( \\textbf S_0^{-1} \\textbf w_0 + \n",
    "\\frac{\\bf \\Phi^T \\textbf t}{\\sigma^2} \\Big) $$\n",
    "\n",
    "$$ \\textbf S_N^{-1} = \\textbf S_0^{-1} + \\frac{\\bf \\Phi^T \\bf \\Phi}{\\sigma^2} $$\n",
    "\n",
    "Let's analyze the results. If we consider the uninformative prior, so $ \\textbf S_0 \\rightarrow \\infty $, and substitute it into the equations before we obtain that the MAP is equal to the maximum likelihood solution of the previous sections. Since the MAP of a Gaussian is equal to the mean:\n",
    "\n",
    "$$ \\lim_{\\textbf S_0 \\to \\infty} \\textbf w_{MAP} = (\\bf \\Phi^T \\bf \\Phi)^{-1} \\bf \\Phi^T \\textbf t = \\textbf w_{ML} $$\n",
    "\n",
    "If then someone will provide us other data, as in sequential data, the just computed posterior acts as a prior for the new model.\n",
    "\n",
    "What if the mean is $0$? Consider a case in which the prior is defined by a single parameter $\\tau^2$:\n",
    "\n",
    "$$ p(\\textbf w| \\tau^2) = \\mathcal N (\\textbf w | \\textbf 0, \\tau^2 \\textbf I)$$\n",
    "\n",
    "we have that the corresponding posterior will have mean and covariance:\n",
    "\n",
    "$$ \\textbf m_N = \\frac{\\textbf S_N \\bf \\Phi^T \\textbf t}{\\sigma^2}$$\n",
    "\n",
    "$$\\textbf S_N^{-1} = \\tau^{-2} \\textbf I + \\sigma^{-2} \\bf \\Phi^T \\bf \\Phi $$\n",
    "\n",
    "from which we can compute the log of the posterior:\n",
    "\n",
    "$$ \\ln p(\\textbf w |\\textbf t) = -\\frac{1}{2 \\sigma^2} \\sum_{n=1}^N(t_n - \\textbf w^T \\phi(\\textbf x_n))^2 - \\frac{1}{2 \\tau^2} \\left\\lVert \\textbf w \\right\\rVert_2^2 $$\n",
    "\n",
    "This is like in regularization since we say that we want parameters that are around zero and the role of $\\lambda$ is played by the covariance matrix of the prior. The maximization of the posterior distribution w.r.t $\\textbf w$ is therefore equivalent to the minimization of the sum-of-squares-error with the addition of a quadratic regularization term with $\\lambda = \\frac{\\sigma^2}{\\tau^2}$.\n",
    "\n",
    "Let's consider the example of a single input variable $x$ and a single target variable $t$ given by a linear model of the form $ y(x, \\textbf w) = w_0 + w_1x$. We can generate synthetic data from the function $ y(x, \\textbf a) = a_0 + a_1x$ with for the example $a_0 = -0.3$ and $a_1 = 0.5$ following three steps:\n",
    "\n",
    "1. draw $x_n$ from the uniform distribution $U(x|-1,1)$\n",
    "2. evaluate $f(x_n, \\textbf a)$\n",
    "3. add Gaussian noise to obtain $y(x_n, \\textbf a)$\n",
    "\n",
    "At this point our goal is to recover $a_0$ and $a_1$ from the data and we will do that with a bayesian approach. The first line of the figure below represents the situation before data are obtained. In the central column is plotted the prior over the parameters and in right one some function drawn from the prior. In the second line we see the situation after having observed a single point, the blu dot in the right column. Since now we have data we can plot on the left column the likelihood. By multiplying this likelihood with the prior of the previous line and normalizing  we obtain the posterior in the central column of the second raw. We go on in the same way for all the data (this is an example of sequential learning). \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_18.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "We can see in the fourth row that as the number of data increases, the posterior becomes more narrow and in the limit of an infinite number of data points it would be a delta function centered on the true parameters.\n",
    "\n",
    "By taking only the MAP we throw away all the informations about the uncertainty and this is not a good thing. It is better to consider the **posterior predictive distribution**\n",
    "\n",
    "$$ p(t| \\textbf x, \\mathcal D, \\sigma^2) = \\int \\mathcal N(t| \\textbf w^T \\phi(\\textbf x), \\sigma^2) \\mathcal N (\\textbf w| \\textbf w_N, \\textbf S_N) d\\textbf w \\\\\n",
    "= \\mathcal N(t| \\textbf w_N^T \\phi(\\textbf x), \\sigma^2_N(\\textbf x)) $$\n",
    "$$ \\sigma^2_N(\\textbf x) = \\sigma^2 + \\phi(\\textbf x)^T \\textbf S_N \\phi(\\textbf x) $$\n",
    "\n",
    "The variance term is composed by the irreducible noise inside the data and a term related to the uncertainty about the parameters. The first term is fixed and we cannot act on it but, as $N \\rightarrow \\infty$ the second term goes to zero. \n",
    "\n",
    "By considering again the example with the sinusoidal function we can illustrate the predictive distribution for Bayesian linear regression. For each plot on the left column below, the solid red curve represents the mean of the Gaussian predictive distribution and the shaded region spans one standard deviation either side of the mean. On the right column we have functions drawn from the posterior distribution. As we can see increasing the number of samples, the shaded region decreses and the functions drawn from the posterior become more compact. The shaded region is also narrow near the blu dots since are observed point and we have just the noise on the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_19.png?raw=1\" width=\"400\"> \n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_20.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_21.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/pict2/figure2_22.png?raw=1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "When we face a Bayesian linear model we have two challenges:\n",
    "\n",
    "1. specify a suitable model, able to describe well the likelihood\n",
    "2. provide a suitable prior\n",
    "\n",
    "In the frequentist approach we just have to face the first problem. The prior is what makes the Bayesian approach strong and at the same time risky. Another aspect related to the Bayesian treatment is associated to the computational issue of the posterior distribution. We have a closed form solution just with Gaussian distributions and so with unimodal problems. When we need to apply other distributions we lose the closed form solution and we need to rely on:\n",
    "* Analytical integration\n",
    "* Gaussian (Laplace) approximation\n",
    "* Monte Carlo integration\n",
    "* Variational approximation\n",
    "\n",
    "So, what we have learned in this last section?\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/stepyt/machine_learning_notes/blob/master/storage/funny/gaussian.JPG?raw=1\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qd7fWRCfY0r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Linear Models for Regression.ipynb",
   "provenance": [
    {
     "file_id": "1jZ7lrbtjRl8hHZWnDzhVVjqosVHmboJ4",
     "timestamp": 1585233211286
    },
    {
     "file_id": "https://gist.github.com/NahrN/519f16431cba2afe62ca66328a6d52ca#file-linear-models-for-regression-ipynb",
     "timestamp": 1585160398718
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
